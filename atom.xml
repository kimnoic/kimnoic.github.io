<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>kimnoic&#39;s home</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-08-17T06:49:40.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>kimnoic</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>LSTM入门学习</title>
    <link href="http://yoursite.com/2017/08/16/LSTM%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2017/08/16/LSTM入门学习/</id>
    <published>2017-08-16T09:22:13.000Z</published>
    <updated>2017-08-17T06:49:40.000Z</updated>
    
    <content type="html"><![CDATA[<script type="text/x-mathjax-config">
MathJax.Hub.Config({
tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>

<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><p>RNN隐藏层神经元计算公式为  $ s_{t} = f\left( x_{t}U+s_{t-1}W \right) $ 其中U、W是网络的参数，f表示激活函数。RNN隐层神经元的计算由t时刻输入xt，t-1时刻隐层神经元激活st-1作为输入。</p>
<h2 id="LSTM结构图"><a href="#LSTM结构图" class="headerlink" title="LSTM结构图"></a>LSTM结构图</h2><p><img src="/2017/08/16/LSTM入门学习/1.png" alt="lstm"></p>
<h2 id="LSTM核心思想"><a href="#LSTM核心思想" class="headerlink" title="LSTM核心思想"></a>LSTM核心思想</h2><p>关键在于细胞状态，水平线在图上方贯穿运行，只有少量线性交互。LSTM拥有三个门，来保护和控制细胞状态。</p>
<h2 id="对LSTM的理解"><a href="#对LSTM的理解" class="headerlink" title="对LSTM的理解"></a>对LSTM的理解</h2><p>决定我们会从细胞状态中丢弃什么信息，该门会读取\(h_{t-1}\)和\(x_t\)，输出一个在0到1的数值给每个细胞状态\(c_{t-1}\)中的数字。回到语言模型的例子来基于已经看到的预测下一个词，可能包含当前主语的性别，因此正确的带刺可以被选择出来。</p>
<h3 id="忘记门层"><a href="#忘记门层" class="headerlink" title="忘记门层"></a>忘记门层</h3><p><img src="/2017/08/16/LSTM入门学习/2.png" alt="lstm"><br>$f_{t}=\sigma\left(W_{f}\bullet[h_{t-1},x_{t}]+b_{f}\right)$</p>
<p>sigmoid层成“输入层”决定什么值我们将要更新，然后一个tanh层创建一个新的候选值向量，\(\tilde{C}_{t}\)会被加入到状态。</p>
<h3 id="输入门层"><a href="#输入门层" class="headerlink" title="输入门层"></a>输入门层</h3><p><img src="/2017/08/16/LSTM入门学习/3.png" alt="lstm"><br>$i_{t}=\sigma\left(W_{i}\bullet[h_{t-1},x_{t}]+b_{i}\right)$</p>
<p>$\tilde{C}_{t}=tanh\left(W_{C}\bullet[h_{t-1},x_{t}]+b_{C}\right)$</p>
<h3 id="更新细胞状态"><a href="#更新细胞状态" class="headerlink" title="更新细胞状态"></a>更新细胞状态</h3><p><img src="/2017/08/16/LSTM入门学习/4.png" alt="lstm"><br>$C_{t}=f_{t}*C_{t-1}+i_{t}*\tilde{C}_{t}$<br>将$\tilde{C}_{t-1}$更新为$C_{t}$，把旧状态与$f_{t}$相乘，丢弃掉我们需要丢弃的信息，接着加上$i_{t}$和$\tilde{C}_{t}$这就是新的候选值。</p>
<h3 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h3><p><img src="/2017/08/16/LSTM入门学习/6.png" alt="lstm"><br>先运行一个sigmoid层来确定细胞状态的那个部分将输出出去，再通过tanh进行处理(得到一个在-1到1之间的值)并将它和sigmoid门的输出相乘，最终我们仅仅会输出我们确定输出的那部分。</p>
<h2 id="LSTM前向传导"><a href="#LSTM前向传导" class="headerlink" title="LSTM前向传导"></a>LSTM前向传导</h2><p>公式为</p>
<p>$i=\sigma\left( x_{t}U^i+S_{T-1}W^i \right)$</p>
<p>$f=\sigma\left(x_{t}U^f+s_{t-1}W^f\right)$</p>
<p>$o=\sigma\left(x_{t}U^o+s_{t-1}W^f\right)$</p>
<p>$g=tanh\left(x_{t}U^g+s_{t-1}W^g\right)$</p>
<p>$c_{t}=c_{t-1}\circ f+g\circ i$</p>
<p>$c_{t}=tanh\left(c_{t}\right) \circ o$</p>
<p>$tanh\left(x\right)=\frac{e^x-e-x}{e^x+e-x}$</p>
<p>to be continued…</p>
]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/x-mathjax-config&quot;&gt;
MathJax.Hub.Config({
tex2jax: {inlineMath: [[&#39;$&#39;,&#39;$&#39;], [&#39;\\(&#39;,&#39;\\)&#39;]]}
});
&lt;/script&gt;

&lt;script type=&quot;te
    
    </summary>
    
    
      <category term="NLP" scheme="http://yoursite.com/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>多层感知机</title>
    <link href="http://yoursite.com/2017/08/11/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/"/>
    <id>http://yoursite.com/2017/08/11/多层感知机/</id>
    <published>2017-08-11T03:27:48.000Z</published>
    <updated>2017-08-11T04:35:45.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h1><p>激活函数的作用是将非线性引入神经元的输出。bais的作用是为每一个节点挺可训练的常量值。</p>
<h2 id="前馈神经网络"><a href="#前馈神经网络" class="headerlink" title="前馈神经网络"></a>前馈神经网络</h2><ul>
<li>输入节点：输入层，不进行任何计算，仅向隐藏节点传递信息</li>
<li>隐藏节点：隐含层，这些节点进行计算，并将信息从输入几点传递到输出节点，可以没有也可以有多个隐藏层</li>
<li>输出节点：输出层</li>
</ul>
<p>在前馈网络中，信息只单向移动。</p>
<h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><ul>
<li>单层感知机：只能学习线性函数</li>
<li>多层感知机：可以学习非线性函数</li>
</ul>
<h2 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h2><p>简单说，就是从错误中学习。最初，所有的边权重都是随机分配的，对于所有训练数据集的输入，人工神经网络都被激活，并且观察期输出。这些输出会和我们一致的、期望的输出进行比较，误差会传播到上一层。该误差会被标注，权重也会被相应的吊证。该流程重复，直到输出误差低于指定的标准。<br>在分类任务中，我们通常在感知器的输出才能好总使用softmax函数作为激活函数，以保证输出的是概率并且相加等于1.</p>
<ul>
<li>向前传播：将训练样本作为输入，计算出输出层两个输出的概率值</li>
<li>反向传播和权重更新：计算出节点的总误差，并将这些误差用反向传播算法传播回网络中，以计算梯度。接下来使用类似梯度下降的算法来调整网络中的所有权重，目的是减少输出层的误差。</li>
</ul>
<p><a href="http://home.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html" target="_blank" rel="external">具体过程可以点击这里</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;多层感知机&quot;&gt;&lt;a href=&quot;#多层感知机&quot; class=&quot;headerlink&quot; title=&quot;多层感知机&quot;&gt;&lt;/a&gt;多层感知机&lt;/h1&gt;&lt;p&gt;激活函数的作用是将非线性引入神经元的输出。bais的作用是为每一个节点挺可训练的常量值。&lt;/p&gt;
&lt;h2 id=&quot;前
    
    </summary>
    
    
      <category term="dl" scheme="http://yoursite.com/tags/dl/"/>
    
  </entry>
  
  <entry>
    <title>CNN</title>
    <link href="http://yoursite.com/2017/08/11/CNN-1/"/>
    <id>http://yoursite.com/2017/08/11/CNN-1/</id>
    <published>2017-08-11T02:49:08.000Z</published>
    <updated>2017-08-16T08:14:09.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h1><p>在图像平整化完全破坏了它的序列，需要一种方式在没有平整haul的情况下把图片馈送给网络，并且还要保留空间排列特征。</p>
<ul>
<li>卷积层：在这一层中，我们定义一个权值矩阵，用来从图像中提取一定的特征。</li>
<li>池化层：有时图像太大，我们需要减少训练参数的数量，池化的目的是减少图像的空间大小。池化在每一个纵深维度上独自完成，因此图像的纵深保持不变，常见形式是最大池化。</li>
<li>输出层：卷积和池化只会提取体征，并减少原始图像带来的参数。输出层具有类似分类交叉熵的损失函数，用于计算预测误差。一旦前向传播完成，反向出阿伯就会开始更新权重和偏差以达到减少误差的目的。</li>
</ul>
<p>下面是整个网络的样子<br><img src="/2017/08/11/CNN-1/16.jpg" alt="16"><br>CNN 中的输出层是全连接层，其中来自其他层的输入在这里被平化和发送，以便将输出转换为网络所需的参数。随后输出层会产生输出，这些信息会互相比较排除错误。损失函数是全连接输出层计算的均方根损失。随后我们会计算梯度错误。错误会进行反向传播，以不断改进过滤器（权重）和偏差值。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;CNN&quot;&gt;&lt;a href=&quot;#CNN&quot; class=&quot;headerlink&quot; title=&quot;CNN&quot;&gt;&lt;/a&gt;CNN&lt;/h1&gt;&lt;p&gt;在图像平整化完全破坏了它的序列，需要一种方式在没有平整haul的情况下把图片馈送给网络，并且还要保留空间排列特征。&lt;/p&gt;
&lt;ul&gt;
    
    </summary>
    
    
      <category term="dl" scheme="http://yoursite.com/tags/dl/"/>
    
  </entry>
  
  <entry>
    <title>some skills to train</title>
    <link href="http://yoursite.com/2017/08/11/some-skills-to-train-1/"/>
    <id>http://yoursite.com/2017/08/11/some-skills-to-train-1/</id>
    <published>2017-08-11T00:27:33.000Z</published>
    <updated>2017-08-16T08:36:37.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一些深度学习的训练技巧"><a href="#一些深度学习的训练技巧" class="headerlink" title="一些深度学习的训练技巧"></a>一些深度学习的训练技巧</h1><h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>深度网络学习过程本质就是为了学习数据分布，网络在每次迭代都是去学习适应不同的分布，这样将会大大降低网络的训练速度。前面基层的输入数据的分布放生改变，那么后面基层就会被累计放大下去。BN就是为了解决训练过程中，中间层数据分布发生变化的情况。简单的做法就是在每一层输入的时候，插入一个归一化层，然后再进入网络的下一层。</p>
<p><img src="/2017/08/11/some-skills-to-train-1/20160312181715397.png" alt="d"></p>
<p>E(x)指每一批训练数据神经元xk的平均值，分母是每一批数据神经元xk激活度的一个标准差。<br>可是随意变换网络的结构会造成其他层学习到的特征分布被搞坏了，<br>利用这个公式，可以学习到其他层的特征分布。</p>
<p><img src="/2017/08/11/some-skills-to-train-1/20160312190113493.png" alt="d"><br><img src="/2017/08/11/some-skills-to-train-1/13.png" alt="d"><br><img src="/2017/08/11/some-skills-to-train-1/14.png" alt="d"><br>在CNN中，我们可以把每个特征图看成是一个特征处理，就是求取所有样本所对应的一个特征图的所有神经元的平均值、方差，然后对这个神经元做归一化。</p>
<h2 id="Deep-Residual-Network"><a href="#Deep-Residual-Network" class="headerlink" title="Deep Residual Network"></a>Deep Residual Network</h2><p>深度学习网络并非是deeper and better，残差网络是一个用来训练非常深的深度网络又十分简洁的框架。在神经网络中，需要反向传播来对网络的权重进行调整，但网络很深的时候就会造成每层损失函数求偏导连乘造成精度很小出现误差。<br><img src="/2017/08/11/some-skills-to-train-1/15.jpg" alt="d"><br>通过求偏导数能看到<br><img src="/2017/08/11/some-skills-to-train-1/equation.png" alt="d"><br>这样就算深度很深，梯度也不会消失。换一个角度看，这也是一种把高阶特征和低阶特征再做融合，从而得到更好的效果。</p>
<h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>说实话，没有读论文，感觉这个东西非常玄学。<br>dropout并不依对代价函数的修改，而是在弃权中改变网络自身。Dropout会临时消除网络中一半的隐藏层神经元，同时让输入层和输出层的神经元保持不变。<br>在前向传播的时候，输入通过修改后的网络，然后反向传播结果。同样通过这个修改后的网络，选择一个新的随机的隐藏神经元的自己进行删除，估计对一个不同的小批量数据的梯度，然后更新权重和偏置。<br>弃掉不同的神经元集合时，像是在训练不同的网络，最后形成的实质就是由若干个子网络组成的新网络。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;一些深度学习的训练技巧&quot;&gt;&lt;a href=&quot;#一些深度学习的训练技巧&quot; class=&quot;headerlink&quot; title=&quot;一些深度学习的训练技巧&quot;&gt;&lt;/a&gt;一些深度学习的训练技巧&lt;/h1&gt;&lt;h2 id=&quot;Batch-Normalization&quot;&gt;&lt;a href=
    
    </summary>
    
    
      <category term="dl" scheme="http://yoursite.com/tags/dl/"/>
    
  </entry>
  
  <entry>
    <title>3/4 of my university</title>
    <link href="http://yoursite.com/2017/08/10/3-4-of-my-university/"/>
    <id>http://yoursite.com/2017/08/10/3-4-of-my-university/</id>
    <published>2017-08-10T12:06:30.000Z</published>
    <updated>2017-08-10T15:01:37.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="想一想自己的大学好像快过去了"><a href="#想一想自己的大学好像快过去了" class="headerlink" title="想一想自己的大学好像快过去了"></a>想一想自己的大学好像快过去了</h1><p>从几个方面谈谈目前为止的大学吧</p>
<h2 id="ACM-amp-Algorithm"><a href="#ACM-amp-Algorithm" class="headerlink" title="ACM &amp; Algorithm"></a>ACM &amp; Algorithm</h2><p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;好像也没有什么好说的，到最后还是没有拿到一块金牌，怎么看自己都挺失败的。不管怎么样，失败的话说了再多也不会影响之前的失败。那还是思考思考惨败的原因吧。什么人曾经说过失败的路有一千条，而成功的路只有一条。仔细想想每天的放松和借口，现在看来总是五味杂陈。谈到失败，我总是想用共性的原因来分析所有事，沉迷于其他不重要的事。反过来思考的时候总是能发现自己的错误，可当时为什么就对自己要求的很松呢。刷题的时候，可能刚刚A了一道，就去看看新闻啊，水水群，逛逛知乎。导致好像时间还都挺满，但实质性的事情却是没怎么做。这么沉重的话还是少说，毕竟ACM也带给我快乐，保持经常刷题也是曾经一个ACMer的基本吧。</p>
<h2 id="出国-保研"><a href="#出国-保研" class="headerlink" title="出国 | 保研"></a>出国 | 保研</h2><p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;这两件事成了我先阶段很头疼的事情。虽说出国一直是我很想的事，但仔细想想，GPA只有3.4多，没有paper，Toefl 和 GRE都没有考出够的分数，那我又拿什么出国？不过现在看，如果搞定了英语成绩，能申请到一个正常大学的MS还是很有希望的。PHD应该是没啥可能吧。那出国的路线就变成一所还算正常的学校-&gt;MS期间刷题&amp;找实习-&gt;运气好能找到一份工作,大概还是做一些码农的事情-&gt;过几年回国-&gt;？出国虽然充满了未知和挑战，同样也充满了艰难的地方。虽然家里还是很愿意提供我出国的花销，但对我这有的工薪家庭来说无疑不是一笔小数目。这不能不被考虑。从一些学长和网上了解到，如果能读个PHD可能会有更大的作为吧。有个老师跟我说过，你最好想清楚自己想要什么，有些人不知道自己该干什么就一直读到博士也不知道适不适合。如果不出国的话，就先在本校读个MS，毕业去个名校读PHD。不过日常的学习英语也是必要的，准备最近找老司机们谈一下，做个最终决定。</p>
<h2 id="Intern"><a href="#Intern" class="headerlink" title="Intern"></a>Intern</h2><p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;来讯飞实习已经一周了，除了读代码之外就是补习机器学习深度学习的一些知识和训练技巧吧。周围有不少学长，感觉大家都很和善，还算是一个不错的事情。不过，一直没有什么实质性的工作，还是搞的我有些不自在。至少有的时候连加班都不知道自己该干些什么，只能快点读完代码，然后赶紧投入到实质性的工作中去吧。其实选择了一个NLP相关的公司，就是为了能多学习一些AI相关的知识。在我看来，能作为一个软件工程师不是长久之计，最好还是能有一些自己比较擅长的事情。</p>
<h2 id="Sports-amp-Gym"><a href="#Sports-amp-Gym" class="headerlink" title="Sports &amp; Gym"></a>Sports &amp; Gym</h2><p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;之前有过想自己做饭的想法，但每天都是有点懒。半年1600的健身房有些贵，但要是能有好身材&amp;好体格还是值得的。这周研究一下伙食的事情，毕竟三分练，七分吃嘛= =</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;想一想自己的大学好像快过去了&quot;&gt;&lt;a href=&quot;#想一想自己的大学好像快过去了&quot; class=&quot;headerlink&quot; title=&quot;想一想自己的大学好像快过去了&quot;&gt;&lt;/a&gt;想一想自己的大学好像快过去了&lt;/h1&gt;&lt;p&gt;从几个方面谈谈目前为止的大学吧&lt;/p&gt;
&lt;h
    
    </summary>
    
    
      <category term="life" scheme="http://yoursite.com/tags/life/"/>
    
  </entry>
  
  <entry>
    <title>a new try</title>
    <link href="http://yoursite.com/2017/08/10/H/"/>
    <id>http://yoursite.com/2017/08/10/H/</id>
    <published>2017-08-10T07:56:07.000Z</published>
    <updated>2017-08-10T11:54:38.000Z</updated>
    
    <content type="html"><![CDATA[<p>hello world~</p>
<h1 id="It’s-a-try-of-Markdown"><a href="#It’s-a-try-of-Markdown" class="headerlink" title="It’s a try of Markdown"></a>It’s a try of Markdown</h1><h2 id="what-is-Markdown"><a href="#what-is-Markdown" class="headerlink" title="what is Markdown"></a>what is Markdown</h2><p>很遗憾，我只是为了试一下Hexo了解了一下这个东西</p>
<ul>
<li>还是？</li>
<li>what？</li>
<li>Okay</li>
</ul>
<blockquote>
<p>忽略这里，这是什么鬼啊</p>
</blockquote>
<p><a href="https://kimnoic.github.io" target="_blank" rel="external">可以点这里</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">#include &lt;bits/stdc++.h&gt;</div><div class="line">using namespace std;</div><div class="line">int main()&#123;</div><div class="line">	cout&lt;&lt;&quot;Winter is coming&quot;;</div><div class="line">	return 0;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="WTF"><a href="#WTF" class="headerlink" title="WTF?"></a>WTF?</h2><p>不对，我好像还在实习呀= =</p>
<p>溜了溜了</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;hello world~&lt;/p&gt;
&lt;h1 id=&quot;It’s-a-try-of-Markdown&quot;&gt;&lt;a href=&quot;#It’s-a-try-of-Markdown&quot; class=&quot;headerlink&quot; title=&quot;It’s a try of Markdown&quot;&gt;&lt;/a&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2017/08/10/hello-world/"/>
    <id>http://yoursite.com/2017/08/10/hello-world/</id>
    <published>2017-08-10T07:07:37.000Z</published>
    <updated>2017-08-10T07:07:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
    
    </summary>
    
    
  </entry>
  
</feed>
